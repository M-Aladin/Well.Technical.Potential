<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Text Mining for Petroleum Engineering: Modern Techniques for Paper Research</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Well.Technical.Potential</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/f0nzie/Well.Technical.Potential">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Text Mining for Petroleum Engineering: Modern Techniques for Paper Research</h1>

</div>


<p><img src="images/cover.jpg" width="800px" style="display: block; margin: auto;" /></p>
<p>Last week I was talking with a colleague about some projects that could be deployed using R, and then the topic of well <strong>technical potential</strong> came up. I got hooked by the subject, and then decided to explore it a little bit more.</p>
<p>What is not better if going to <a href="https://www.onepetro.org/">OnePetro</a> and search for papers on <strong>technical potential</strong>. I did and found 132 papers matching the term. Well, I was not going to purchase those 132 papers, you know. I had to reduce that number to an essential minimum. How?</p>
<p>That is the topic of today’s article. And for that purpose, we will be using <a href="https://en.wikipedia.org/wiki/Text_mining">text mining</a>.</p>
<div id="the-toolbox" class="section level2">
<h2>The Toolbox</h2>
<p>I will be using the following ingredients for this recipe. :)</p>
<ul>
<li><p>The R package for text mining <a href="https://cran.r-project.org/web/packages/tm/index.html">tm</a></p></li>
<li><p>My R package <a href="https://cran.r-project.org/web/packages/petro.One/index.html">petro.One</a></p></li>
<li><p><a href="https://www.rstudio.com/">RStudio</a></p></li>
<li><p>The graphics R package <a href="https://cran.r-project.org/web/packages/ggplot2/index.html">ggplot2</a></p></li>
</ul>
<p>All of these tools are open source and free. Never has been more true saying “free ride” for studying, learning and using state-of-the-art software than today. You will just have to invest some of your time.</p>
</div>
<div id="how-many-papers-are-there" class="section level2">
<h2>How many papers are there?</h2>
<p>Let’s find out how many papers contain the term <strong>technical potential</strong>. Here is the code in R:</p>
<pre class="r"><code>library(petro.One)

my_url &lt;- make_search_url(query = &quot;technical potential&quot;, 
                           how = &quot;all&quot;)
num_papers &lt;- get_papers_count(my_url)
# [1] 132</code></pre>
<p>What the package <code>petro.One</code> does is connecting with <code>OnePetro</code> and submit the text <strong>technical potential</strong> to the website, specifying that we want papers that only have those two words together with the parameter <code>how</code>. We could read the number of papers from the object <code>num_papers</code>.</p>
<p>We read the titles of the papers, and the rest of the metadata, and put that in a dataframe <code>df</code>. So, we get 132 papers as it is shown below.</p>
<pre class="r"><code>df &lt;- read_multidoc(my_url)
df</code></pre>
<pre><code># A tibble: 132 x 6
                                                                    title_data
                                                                         &lt;chr&gt;
 1 Reserve Technical Potential Management System: Key in Rehabilitation of Mat
 2 Technical Potential: A Highly Effective Tool for National Oil Companies to 
 3 Using Business Intelligent Tool to Gain Insight into Well Integrity, Survei
 4 Managed Pressure Drilling With Solids-Free Drilling Fluid Provides Cost-Eff
 5                           Optimizing Number of Fractures in Horizontal Well
 6 Intelligent Automation of Mature Field Development Process: Case Study From
 7 Staying Ahead through Built-in Automated Approach for Well Modelling in Gas
 8                                   The Models of Sea Waves Energy Converters
 9   The First Subsea Completion by an Operator in KN Field, Offshore Malaysia
10         Surfactant Flooding: Technical and Economical Conditions To Succeed
# ... with 122 more rows, and 5 more variables: paper_id &lt;chr&gt;,
#   source &lt;chr&gt;, type &lt;chr&gt;, year &lt;int&gt;, author1_data &lt;chr&gt;</code></pre>
</div>
<div id="how-many-papers-with-technical-and-potential" class="section level2">
<h2>How many papers with “technical” and “potential”</h2>
<p>We assume that the papers with stronger <strong>technical potential</strong> content are those that have in the title those words. We will look at papers in the title.</p>
<pre class="r"><code>df$title &lt;- tolower(df$title_data)
# df[grep(&quot;technical potential&quot;, df$title), c(&quot;title_data&quot;, &quot;paper_id&quot;)]
df[grep(&quot;technical potential&quot;, df$title), ]</code></pre>
<pre><code># A tibble: 3 x 7
                                                                   title_data
                                                                        &lt;chr&gt;
1 Reserve Technical Potential Management System: Key in Rehabilitation of Mat
2 Technical Potential: A Highly Effective Tool for National Oil Companies to 
3 Technical Potential of Methods for Methane Extraction From Geopressured-Geo
# ... with 6 more variables: paper_id &lt;chr&gt;, source &lt;chr&gt;, type &lt;chr&gt;,
#   year &lt;int&gt;, author1_data &lt;chr&gt;, title &lt;chr&gt;</code></pre>
<p>There are 3 papers that have <code>technical potential</code> in their title.</p>
<p>At this point, we could do two things:</p>
<ul>
<li>retrieve the papers that have <code>technical potential</code> in the title</li>
<li>perform an additional 1-word and 2-word terms analysis on the 132 that match our query.</li>
</ul>
<p>We will do first a text mining of the papers that gave a match on the title.</p>
<ul>
<li>What grep does is finding a word pattern in the column title of the dataframe <code>df</code>. There are 3 papers that match that pattern.</li>
</ul>
<pre class="r"><code>df[grep(&quot;technical potential&quot;, df$title), c(&quot;source&quot;, &quot;paper_id&quot;)]</code></pre>
<pre><code># A tibble: 3 x 2
          source             paper_id
           &lt;chr&gt;                &lt;chr&gt;
1           SPE            169835-MS 
2           SPE            187429-MS 
3           SPE              9469-PA </code></pre>
<p>These three papers are our best candidates, from the 132 papers that matched our initial query. Next is finding which of the three papers is the one richer in <strong>technical potential</strong> content and focus our attention to it.</p>
<p>What we will do is reading inside the papers, the PDF files.</p>
</div>
<div id="data-mining-the-pdf-files" class="section level2">
<h2>Data Mining the PDF files</h2>
<p>Once the papers have been downloaded, we will verify that they are in our working directory:</p>
<pre class="r"><code>files &lt;- list.files(pattern = &quot;.pdf$&quot;)
files</code></pre>
<pre><code>[1] &quot;gupta2017.pdf&quot;  &quot;quong1982.pdf&quot;  &quot;ruslan2014.pdf&quot;</code></pre>
<p>That gives us three PDF files. Pay attention to the object files. We will use it to create the mining corpus in a moment.</p>
</div>
<div id="read-the-pdf-files-and-inspect-the-corpus" class="section level2">
<h2>Read the PDF files and inspect the corpus</h2>
<p>The following operation is reading the papers that are in PDF format. <code>R</code> has a way to read PDF files through the function <code>readPDF</code> of the package <code>tm</code>.</p>
<pre class="r"><code>library(tm)

Rpdf &lt;- readPDF(control = list(text = &quot;-layout&quot;))
papers &lt;- Corpus(URISource(files), 
                   readerControl = list(reader = Rpdf))
papers &lt;- tm_map(papers, content_transformer(function(x) iconv(enc2utf8(x), 
                                                                 sub = &quot;byte&quot;)))

inspect(papers)

# one-word terms in gupta2017 paper
papers.tdm &lt;- TermDocumentMatrix(papers, 
                                    control = list(removePunctuation = TRUE, 
                                                   stopwords = TRUE,
                                                   removeNumbers = TRUE,
                                                   tolower = TRUE
                                                   #stemming = TRUE
                                                   ))
inspect(papers.tdm)</code></pre>
<pre><code>&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 3

[[1]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 58402

[[2]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 50389

[[3]]
&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  7
Content:  chars: 27550

&lt;&lt;TermDocumentMatrix (terms: 3424, documents: 3)&gt;&gt;
Non-/sparse entries: 4141/6131
Sparsity           : 60%
Maximal term length: 48
Weighting          : term frequency (tf)
Sample             :
             Docs
Terms         gupta2017.pdf quong1982.pdf ruslan2014.pdf
  based                  36             3              7
  data                   77            10              8
  field                  27             0             17
  fields                 42             0              5
  forecast               68             0              0
  forecasting            51             0              0
  potential              52             7             40
  production             68             7             23
  technical              47             4             36
  well                   40             4             10</code></pre>
<p>The <code>papers</code> object is the <em>corpus</em>. The object <code>papers.tdm</code> is the <em>term document matrix</em>.</p>
<p>The table at the bottom is the <em>term document matrix</em>. A matrix where the rows are the terms and the columns are the counts of those terms for each paper.</p>
<p>Observe that each of the PDF files has an identifier like this <code>[[1]]</code>, <code>[[2]]</code> and <code>[[3]]</code>. Take a look at the number of characters each document contains.</p>
<p>Now, let’s get the number of words or terms</p>
<pre class="r"><code># how many terms per paper
sapply(papers, function(x) length(termFreq(x)) )</code></pre>
<pre><code> gupta2017.pdf  quong1982.pdf ruslan2014.pdf 
          2274           1882           1207 </code></pre>
<p>Now, a summary table of our initial findings:</p>
<pre><code>Docs        gupta2017.pdf quong1982.pdf ruslan2014.pdf
Num Chars      58230           50343        27492
Terms           2274            1878         1207</code></pre>
<p>We can see that the document with more content is <code>gupta2017</code>, the second <code>quong1982</code>, and the third, <code>ruslan2014</code>. But we will find something interesting later.</p>
</div>
<div id="find-the-most-frequent-terms-in-the-papers" class="section level2">
<h2>Find the most frequent terms in the papers</h2>
<p>Now that the papers corpus has been converted to a term document matrix, we could continue with finding the most frequent terms:</p>
<pre class="r"><code># findFreqTerms(papers.tdm, lowfreq = 50, highfreq = Inf)
findMostFreqTerms(papers.tdm)</code></pre>
<pre><code>$gupta2017.pdf
       data    forecast  production   potential forecasting   technical 
         77          68          68          52          51          47 

$quong1982.pdf
  pressure solubility      brine        gas  injection      freon 
        38         32         31         27         25         21 

$ruslan2014.pdf
  reserves  potential  technical production       will      field 
        41         40         36         23         20         17 </code></pre>
<p>What is happening here is that even though “technical potential” is in the title of <code>quong1982</code>, the paper is not strong in technical potential, per se. The other two papers are stronger.</p>
<p>Observe the frequency of the terms. What is happening here is that even though “technical potential” is found in the title of the paper quong1982, the paper is not rich in technical potential terms. The other two papers are stronger. We will put aside <code>quong1982</code> and analyze <code>gupta2017</code> and <code>ruslan2014</code>.</p>
<p>Just to be sure, one more time let’s find the score of all the papers given the terms “technical” and “potential”.</p>
<pre class="r"><code>tm_term_score(papers.tdm, &quot;technical&quot;)
tm_term_score(papers.tdm, &quot;potential&quot;)</code></pre>
<pre><code> gupta2017.pdf  quong1982.pdf ruslan2014.pdf 
            47              4             36 
 gupta2017.pdf  quong1982.pdf ruslan2014.pdf 
            52              7             40 </code></pre>
<p>Well, that confirms our initial analysis; <code>quong1982</code> is not the best candidate paper for studying “technical potential”.</p>
</div>
<div id="term-frequency-analysis-for-the-first-paper" class="section level2">
<h2>Term frequency analysis for the first paper</h2>
<p>What we do here is building a dataframe of terms vs. frequency at which each occur.</p>
<pre class="r"><code>library(tibble)

# frequency analysis of gupta2017
theFile &lt;- &quot;gupta2017.pdf&quot;
paper &lt;- Corpus(URISource(theFile), 
                   readerControl = list(reader = Rpdf))
paper &lt;- tm_map(paper, content_transformer(function(x) iconv(enc2utf8(x), 
                                                                 sub = &quot;byte&quot;)))

paper.tdm &lt;- TermDocumentMatrix(paper, 
                                    control = list(removePunctuation = TRUE, 
                                                   stopwords = TRUE,
                                                   removeNumbers = TRUE,
                                                   tolower = TRUE
                                                   #stemming = TRUE
                                                   ))
findFreqTerms(paper.tdm, lowfreq = 50, highfreq = Inf)
findMostFreqTerms(paper.tdm)

tdm.matrix &lt;- as.matrix(paper.tdm)
tdm.rs &lt;- sort(rowSums(tdm.matrix), decreasing = TRUE)
tdm.df1 &lt;- tibble(word = names(tdm.rs), freq = tdm.rs)
tdm.df1</code></pre>
<pre><code>[1] &quot;data&quot;        &quot;forecast&quot;    &quot;forecasting&quot; &quot;potential&quot;   &quot;production&quot; 
$gupta2017.pdf
       data    forecast  production   potential forecasting   technical 
         77          68          68          52          51          47 

# A tibble: 1,823 x 2
          word  freq
         &lt;chr&gt; &lt;dbl&gt;
 1        data    77
 2    forecast    68
 3  production    68
 4   potential    52
 5 forecasting    51
 6   technical    47
 7      fields    42
 8       model    41
 9        well    40
10       based    36
# ... with 1,813 more rows</code></pre>
</div>
<div id="term-freqency-analysis-for-the-second-paper" class="section level1">
<h1>Term freqency analysis for the second paper</h1>
<p>Now, for the second paper:</p>
<pre class="r"><code># frequency analysis of ruslan2014
theFile &lt;- &quot;ruslan2014.pdf&quot;
paper &lt;- Corpus(URISource(theFile), 
                   readerControl = list(reader = Rpdf))
paper &lt;- tm_map(paper, content_transformer(function(x) iconv(enc2utf8(x), 
                                                                 sub = &quot;byte&quot;)))

paper.tdm &lt;- TermDocumentMatrix(paper, 
                                    control = list(removePunctuation = TRUE, 
                                                   stopwords = TRUE,
                                                   tolower = TRUE,
                                                   removeNumbers = TRUE
                                                   #stemming = TRUE
                                                   ))
findFreqTerms(paper.tdm, lowfreq = 50, highfreq = Inf)
findMostFreqTerms(paper.tdm)

tdm.matrix &lt;- as.matrix(paper.tdm)
tdm.rs &lt;- sort(rowSums(tdm.matrix), decreasing = TRUE)
tdm.df2 &lt;- tibble(word = names(tdm.rs), freq = tdm.rs)
tdm.df2</code></pre>
<pre><code>character(0)
$ruslan2014.pdf
  reserves  potential  technical production       will      field 
        41         40         36         23         20         17 

# A tibble: 999 x 2
         word  freq
        &lt;chr&gt; &lt;dbl&gt;
 1   reserves    41
 2  potential    40
 3  technical    36
 4 production    23
 5       will    20
 6      field    17
 7     mature    16
 8 activities    15
 9 management    15
10  resources    15
# ... with 989 more rows</code></pre>
<p>We can see some differences between the two: such as the total number of terms or words, the frequency for each term, and that the terms and frequency differ in both documents.</p>
<p>To finish, let’s plot terms vs frequency for both papers:</p>
<pre class="r"><code>library(ggplot2)
p1 &lt;- ggplot(subset(tdm.df1, freq &gt; 30), aes(x=word, y=freq)) + 
    geom_bar(stat = &quot;identity&quot;, width = 0.8) + 
    xlab(&quot;Terms&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;gupta2017&quot;) +
    coord_flip()

p2 &lt;- ggplot(subset(tdm.df2, freq &gt; 30), aes(x=reorder(word, freq), y=freq)) + 
    geom_bar(stat = &quot;identity&quot;, width = 0.2) + 
    xlab(&quot;Terms&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;ruslan2014&quot;) +
    coord_flip()

require(&quot;gridExtra&quot;)
grid.arrange(arrangeGrob(p1, p2))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<ul>
<li><p>We rapidly determined, in our study for the term “technical potential”, which papers are the best candidates.</p></li>
<li><p>We used text mining, composed a document corpus and a term document matrix, to take that decision, and narrow down from 132 to 3 papers that search and the acquisition of those papers.</p></li>
<li><p>From the three selected papers, we were able to put aside one because the other two papers were even richer in the content we were looking for.</p></li>
</ul>
<p>This doesn’t mean that we should discard reading the other 130 papers; but doing a deeper analysis of all the papers requires purchasing and downloading all of them. If this option is viable, we may find that other papers may contain interesting content for the subject of our research. In the case of our study for reasons of time and budget we came up rapidly to only those two papers.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
